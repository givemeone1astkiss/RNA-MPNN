{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T07:26:59.393853Z",
     "start_time": "2025-03-24T07:26:59.165199Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 2317/2317 [00:00<00:00, 63597.47file/s]\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "No protocol specified\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There is at least non-excluded one OpenFabrics device found,\n",
      "but there are no active ports detected (or Open MPI was unable to use\n",
      "them).  This is most certainly not what you wanted.  Check your\n",
      "cables, subnet manager configuration, etc.  The openib BTL will be\n",
      "ignored for this job.\n",
      "\n",
      "  Local host: jinhw-Precision-7960-Tower\n",
      "--------------------------------------------------------------------------\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "You are using a CUDA device ('NVIDIA RTX A5500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/zhangliqin/RNA-MPNN/out/checkpoints/RDesign exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | features       | RNAFeatures      | 56.8 K\n",
      "1 | encoder_layers | ModuleList       | 3.4 M \n",
      "2 | decoder_layers | ModuleList       | 3.4 M \n",
      "3 | readout        | Linear           | 1.0 K \n",
      "4 | loss_fn        | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "6.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 M     Total params\n",
      "27.593    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f545f53c6e454cb7052079edcdb427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 1] Received SIGTERM: 15\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1031, in _run_stage\n    self._run_sanity_check()\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1060, in _run_sanity_check\n    val_loop.run()\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n    return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 392, in validation_step\n    logits, S = self(X, S, mask)\n                ^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 341, in forward\n    X, S, h_V, h_E, E_idx, batch_id = self.features(X, S, mask)\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 227, in forward\n    D_neighbors, E_idx = self._dist(X_backbone, mask)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 101, in _dist\n    D = (1. - mask_2D) * 10000 + mask_2D * torch.sqrt(torch.sum(dX ** 2, 3) + eps)\n         ~~~^~~~~~~~~\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 40, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 941, in __rsub__\n    return _C._VariableFunctions.rsub(self, other)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.00 MiB is free. Process 2724649 has 21.06 GiB memory in use. Including non-PyTorch memory, this process has 2.37 GiB memory in use. Of the allocated memory 1.95 GiB is allocated by PyTorch, and 23.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m RNADataModule\u001b[38;5;241m.\u001b[39mfrom_defaults()\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRDesign\u001b[39m\u001b[38;5;124m'\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py:144\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m process_context \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mstart_processes(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39mprocess_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we will join ourselves to get the process references\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocs \u001b[38;5;241m=\u001b[39m process_context\u001b[38;5;241m.\u001b[39mprocesses\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mprocess_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    147\u001b[0m worker_output \u001b[38;5;241m=\u001b[39m return_queue\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:158\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    156\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    157\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 68, in _wrap\n    fn(i, *args)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 580, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 987, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1031, in _run_stage\n    self._run_sanity_check()\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1060, in _run_sanity_check\n    val_loop.run()\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 182, in _decorator\n    return loop_run(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 135, in run\n    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 396, in _evaluation_step\n    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 309, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n    return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 642, in __call__\n    wrapper_output = wrapper_module(*args, **kwargs)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1523, in forward\n    else self._run_ddp_forward(*inputs, **kwargs)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/parallel/distributed.py\", line 1359, in _run_ddp_forward\n    return self.module(*inputs, **kwargs)  # type: ignore[index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 635, in wrapped_forward\n    out = method(*_args, **_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 392, in validation_step\n    logits, S = self(X, S, mask)\n                ^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 341, in forward\n    X, S, h_V, h_E, E_idx, batch_id = self.features(X, S, mask)\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 227, in forward\n    D_neighbors, E_idx = self._dist(X_backbone, mask)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/RNA-MPNN/rnampnn/model/rdesign.py\", line 101, in _dist\n    D = (1. - mask_2D) * 10000 + mask_2D * torch.sqrt(torch.sum(dX ** 2, 3) + eps)\n         ~~~^~~~~~~~~\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 40, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/zhangliqin/miniconda3/lib/python3.12/site-packages/torch/_tensor.py\", line 941, in __rsub__\n    return _C._VariableFunctions.rsub(self, other)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 488.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 90.00 MiB is free. Process 2724649 has 21.06 GiB memory in use. Including non-PyTorch memory, this process has 2.37 GiB memory in use. Of the allocated memory 1.95 GiB is allocated by PyTorch, and 23.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "from rnampnn import *\n",
    "\n",
    "model = RNAModel()\n",
    "data = RNADataModule.from_defaults(batch_size=8)\n",
    "trainer = get_trainer(name='RDesign', version=1, max_epochs=60)\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
